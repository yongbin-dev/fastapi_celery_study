# Celery íƒœìŠ¤í¬ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

## ê°œìš”

ì´ ë¬¸ì„œëŠ” FastAPI + Celery í™˜ê²½ì—ì„œ êµ¬í˜„ëœ íƒœìŠ¤í¬ ëª¨ë‹ˆí„°ë§ ë° íŒŒì´í”„ë¼ì¸ ì¶”ì  ì‹œìŠ¤í…œì˜ ì•„í‚¤í…ì²˜ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.

## ì‹œìŠ¤í…œ êµ¬ì„± ìš”ì†Œ

### 1. í•µì‹¬ ì»´í¬ë„ŒíŠ¸

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Celery Signals    â”‚â”€â”€â”€â”€â”‚   TaskInfo Model    â”‚â”€â”€â”€â”€â”‚   TaskService       â”‚
â”‚   (ì‹¤ì‹œê°„ ì¶”ì )       â”‚    â”‚   (ë°ì´í„° ì˜ì†í™”)     â”‚    â”‚   (API ì¸í„°í˜ì´ìŠ¤)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                          â”‚                          â”‚
           â–¼                          â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL        â”‚    â”‚   Redis Backend     â”‚    â”‚   REST API          â”‚
â”‚   (ë©”íƒ€ë°ì´í„° ì €ì¥)   â”‚    â”‚   (Celery ê²°ê³¼)     â”‚    â”‚   (ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2. ë°ì´í„° ëª¨ë¸

### TaskInfo ëª¨ë¸ êµ¬ì¡°

```sql
CREATE TABLE task_info (
    id SERIAL PRIMARY KEY,
    task_id VARCHAR(255) UNIQUE NOT NULL,     -- Celery íƒœìŠ¤í¬ UUID
    status VARCHAR(255) NOT NULL,             -- STARTED, SUCCESS, FAILURE, etc.
    task_name VARCHAR(255) NOT NULL,          -- ì˜ˆ: app.tasks.ai_pipeline_orchestrator
    args TEXT,                                -- íƒœìŠ¤í¬ ì¸ìˆ˜
    kwargs TEXT,                              -- íƒœìŠ¤í¬ í‚¤ì›Œë“œ ì¸ìˆ˜
    result TEXT,                              -- íƒœìŠ¤í¬ ê²°ê³¼
    error_message TEXT,                       -- ì˜¤ë¥˜ ë©”ì‹œì§€
    traceback TEXT,                           -- ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤
    retry_count INTEGER DEFAULT 0,            -- ì¬ì‹œë„ íšŸìˆ˜
    
    -- ì²´ì¸/íŒŒì´í”„ë¼ì¸ ì§€ì›
    root_task_id VARCHAR,                     -- ì²´ì¸ì˜ ë£¨íŠ¸ íƒœìŠ¤í¬
    parent_task_id VARCHAR,                   -- ì§ì ‘ì ì¸ ë¶€ëª¨ íƒœìŠ¤í¬
    chain_total INTEGER,                      -- ì²´ì¸ì˜ ì „ì²´ íƒœìŠ¤í¬ ìˆ˜
    
    -- íƒ€ì„ìŠ¤íƒ¬í”„
    task_time TIMESTAMP WITH TIME ZONE,      -- íƒœìŠ¤í¬ ì‹œì‘ ì‹œê°„
    completed_time TIMESTAMP WITH TIME ZONE, -- íƒœìŠ¤í¬ ì™„ë£Œ ì‹œê°„
    
    -- ì¸ë±ìŠ¤
    INDEX idx_task_id (task_id),
    INDEX idx_root_task_id (root_task_id),
    INDEX idx_parent_task_id (parent_task_id)
);
```

### ì²´ì¸ êµ¬ì¡° ì˜ˆì‹œ

```
Pipeline: ai_pipeline_orchestrator (root)
â”œâ”€â”€ stage1_preprocessing (parent: root)
â”œâ”€â”€ stage2_feature_extraction (parent: stage1)
â”œâ”€â”€ stage3_model_inference (parent: stage2)
â””â”€â”€ stage4_post_processing (parent: stage3)
```

## 3. ì‹œê·¸ë„ ê¸°ë°˜ ì‹¤ì‹œê°„ ì¶”ì 

### 3.1 Celery ì‹œê·¸ë„ í•¸ë“¤ëŸ¬

```python
# app/core/celery_signals.py

@signals.task_prerun.connect
def task_prerun_handler(task_id, task, args, kwargs, **kwds):
    """íƒœìŠ¤í¬ ì‹œì‘ ì „ - ë°ì´í„°ë² ì´ìŠ¤ì— ì´ˆê¸° ì •ë³´ ì €ì¥"""
    
@signals.task_postrun.connect  
def task_postrun_handler(sender, task_id, task, retval, state, **kwds):
    """íƒœìŠ¤í¬ ì™„ë£Œ í›„ - ê²°ê³¼ ë° ìƒíƒœ ì—…ë°ì´íŠ¸"""
    
@signals.task_failure.connect
def task_failure_handler(sender, task_id, exception, traceback, **kwds):
    """íƒœìŠ¤í¬ ì‹¤íŒ¨ ì‹œ - ì˜¤ë¥˜ ì •ë³´ ì €ì¥"""
    
@signals.task_retry.connect
def task_retry_handler(sender, task_id, reason, **kwds):
    """íƒœìŠ¤í¬ ì¬ì‹œë„ ì‹œ - ì¬ì‹œë„ ì¹´ìš´íŠ¸ ì¦ê°€"""
```

### 3.2 ì²´ì¸ ì •ë³´ ì¶”ì¶œ

```python
def extract_chain_info(task, task_id: str) -> Dict[str, Any]:
    """íƒœìŠ¤í¬ì—ì„œ chain ê´€ë ¨ ì •ë³´ë¥¼ ì¶”ì¶œ"""
    chain_info = {}
    
    if hasattr(task, 'request'):
        # Root task ID (ì²´ì¸ì˜ ì²« ë²ˆì§¸ íƒœìŠ¤í¬)
        if hasattr(task.request, 'root_id'):
            chain_info['root_task_id'] = task.request.root_id
            
        # Parent task ID (ì§ì ‘ì ì¸ ë¶€ëª¨)  
        if hasattr(task.request, 'parent_id'):
            chain_info['parent_task_id'] = task.request.parent_id
            
        # Chain ì •ë³´
        if hasattr(task.request, 'chain'):
            chain_info['chain_total'] = len(task.request.chain) + 1
            
    return chain_info
```

## 4. íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ ì‹œìŠ¤í…œ

### 4.1 í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•

```python
def get_pipeline_status(self, pipeline_id: str) -> PipelineStatusResponse:
    """Redis + PostgreSQL + AsyncResult íŠ¸ë¦¬í”Œ í•˜ì´ë¸Œë¦¬ë“œ ì¡°íšŒ"""
    
    # 1. PostgreSQLì—ì„œ ì²´ì¸ êµ¬ì¡° íŒŒì•…
    chain_task_ids = self._get_chain_task_ids(pipeline_id)
    
    # 2. ê° íƒœìŠ¤í¬ë³„ 3-tier ë°ì´í„° ì¡°íšŒ
    for task_id in chain_task_ids:
        # Tier 1: Redisì—ì„œ task_name + ê¸°ë³¸ ì •ë³´
        redis_data = self._get_redis_task_meta(task_id)
        task_name = redis_data.get('task_name')
        
        # Tier 2: AsyncResultì—ì„œ ì‹¤ì‹œê°„ ìƒíƒœ
        async_result = celery_app.AsyncResult(task_id)
        current_state = async_result.state
        is_ready = async_result.ready()
        
        # Tier 3: PostgreSQLì—ì„œ fallback ì •ë³´
        db_data = self._get_db_task_info(task_id)
        fallback_task_name = db_data.task_name if db_data else None
        
        # ìš°ì„ ìˆœìœ„ ê²°í•©
        final_task_name = task_name or fallback_task_name or 'unknown'
```

### 4.1.2 Redis ë©”íƒ€ë°ì´í„° í–¥ìƒ

```python
# Celery ì‹œê·¸ë„ì—ì„œ Redisì— task_name ìë™ ì €ì¥
@signals.task_prerun.connect
def store_task_name_in_redis_early(task_id: str, task_name: str):
    redis_client.hset(f"celery-task-meta-{task_id}", mapping={
        "task_name": task_name,
        "status": "STARTED",
        "start_time": datetime.now().isoformat()
    })

@signals.task_postrun.connect
def update_redis_task_meta(sender, task_id, task_name, state):
    redis_client.hset(f"celery-task-meta-{task_id}", "task_name", task_name)
```

### 4.2 ë°ì´í„° ì†ŒìŠ¤ë³„ ì—­í• 

| ë°ì´í„° ì†ŒìŠ¤ | ì œê³µí•˜ëŠ” ì •ë³´ | ì¥ì  | ë‹¨ì  |
|-------------|---------------|------|------|
| **PostgreSQL** | ì²´ì¸ ì •ë³´, íˆìŠ¤í† ë¦¬, traceback | ì˜ì†ì , ì •í™•í•¨, ë³µì¡í•œ ì¿¼ë¦¬ | ì‹¤ì‹œê°„ì„± ë¶€ì¡± |
| **Redis (Enhanced)** | ì‹¤ì‹œê°„ ìƒíƒœ, task_name, í˜„ì¬ ê²°ê³¼ | ë¹ ë¥¸ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ | íœ˜ë°œì„± |
| **AsyncResult** | ì •í™•í•œ state, ready() ìƒíƒœ | Celery ë„¤ì´í‹°ë¸Œ | task_name ì—†ìŒ |

### 4.3 ìƒíƒœ ê²°ì • ë¡œì§

```python
# ì „ì²´ íŒŒì´í”„ë¼ì¸ ìƒíƒœ ê²°ì •
if all(task.status == 'SUCCESS' for task in tasks):
    overall_state = 'SUCCESS'
elif any(task.status == 'FAILURE' for task in tasks):
    overall_state = 'FAILURE'  
elif any(task.status in ['PROGRESS', 'STARTED'] for task in tasks):
    overall_state = 'PROGRESS'
else:
    overall_state = 'PENDING'
```

## 5. API ì¸í„°í˜ì´ìŠ¤

### 5.1 íŒŒì´í”„ë¼ì¸ ìƒì„±

```http
POST /api/v1/tasks/ai-pipeline
{
    "text": "ì²˜ë¦¬í•  í…ìŠ¤íŠ¸",
    "options": {},
    "priority": 5
}
```

**ì‘ë‹µ:**
```json
{
    "success": true,
    "data": {
        "pipeline_id": "4fceefce-64d8-41a1-9941-f62aa4152e63",
        "status": "STARTED",
        "message": "AI ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤",
        "estimated_duration": 20
    }
}
```

### 5.2 íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ

```http
GET /api/v1/tasks/ai-pipeline/{pipeline_id}/status
```

**ì‘ë‹µ:**
```json
{
    "success": true,
    "data": {
        "pipeline_id": "4fceefce-64d8-41a1-9941-f62aa4152e63",
        "overall_state": "PROGRESS",
        "total_steps": 2,
        "tasks": [
            {
                "task_id": "4fceefce-64d8-41a1-9941-f62aa4152e63",
                "status": "SUCCESS", 
                "task_name": "app.tasks.ai_pipeline_orchestrator",
                "result": "Pipeline orchestrated successfully",
                "step": 1,
                "ready": true
            },
            {
                "task_id": "b0e5b703-8a53-44fe-9cac-83e5372a655d",
                "status": "PROGRESS",
                "task_name": "app.tasks.stage1_preprocessing", 
                "result": "{\"stage\": 1, \"progress\": 50}",
                "step": 2,
                "ready": false
            }
        ]
    }
}
```

### 5.3 íƒœìŠ¤í¬ íˆìŠ¤í† ë¦¬ ì¡°íšŒ

```http
GET /api/v1/tasks/history?hours=24&status=SUCCESS&limit=100
```

## 6. ì„±ëŠ¥ ìµœì í™”

### 6.1 ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìµœì í™”

```python
class TaskService:
    def __init__(self):
        # ë™ê¸°ì‹ DB ì—°ê²° í’€ (íƒœìŠ¤í¬ ì¡°íšŒìš©)
        sync_db_url = settings.DATABASE_URL.replace("+asyncpg", "")
        self.sync_engine = create_engine(sync_db_url)
        self.SyncSessionLocal = sessionmaker(bind=self.sync_engine)
```

### 6.2 ì¸ë±ìŠ¤ ì „ëµ

```sql
-- ì²´ì¸ ì¡°íšŒ ìµœì í™”
CREATE INDEX idx_root_task_id ON task_info(root_task_id);
CREATE INDEX idx_parent_task_id ON task_info(parent_task_id);

-- ì‹œê°„ ë²”ìœ„ ì¡°íšŒ ìµœì í™”  
CREATE INDEX idx_task_time ON task_info(task_time);

-- ìƒíƒœë³„ ì¡°íšŒ ìµœì í™”
CREATE INDEX idx_status_time ON task_info(status, task_time);
```

### 6.3 ë©”ëª¨ë¦¬ ê´€ë¦¬

- **Context Manager**: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìë™ í•´ì œ
- **Thread-Safe**: RLockì„ ì‚¬ìš©í•œ ë™ì‹œì„± ì œì–´
- **Connection Pool**: SQLAlchemy ì—°ê²° í’€ í™œìš©

## 7. ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### 7.1 êµ¬ì¡°í™”ëœ ë¡œê¹…

```python
logger.info(f"ğŸ”— Chain íƒœìŠ¤í¬: {task_name} "
           f"(Root: {chain_info['root_task_id']}, "
           f"Parent: {chain_info.get('parent_task_id', 'None')}, "
           f"Total: {chain_info.get('chain_total', '?')})")
```

### 7.2 ë©”íŠ¸ë¦­ ìˆ˜ì§‘

- íƒœìŠ¤í¬ ì„±ê³µë¥ 
- í‰ê·  ì‹¤í–‰ ì‹œê°„
- ì²´ì¸ ì™„ë£Œìœ¨
- ì—ëŸ¬ ë°œìƒ ë¹ˆë„

## 8. í™•ì¥ì„± ê³ ë ¤ì‚¬í•­

### 8.1 ìˆ˜í‰ í™•ì¥

- **ë‹¤ì¤‘ ì›Œì»¤**: Celery ì›Œì»¤ ìŠ¤ì¼€ì¼ë§
- **ë°ì´í„°ë² ì´ìŠ¤ ìƒ¤ë”©**: task_id ê¸°ë°˜ ë¶„ì‚°
- **ìºì‹± ë ˆì´ì–´**: Redisë¥¼ í†µí•œ ë¹ˆë²ˆí•œ ì¡°íšŒ ìºì‹±

### 8.2 ê³ ê°€ìš©ì„±

- **Failover**: AsyncResult fallback ë©”ì»¤ë‹ˆì¦˜
- **Data Consistency**: íŠ¸ëœì­ì…˜ì„ í†µí•œ ë°ì´í„° ì¼ê´€ì„±
- **Error Handling**: ê° ë‹¨ê³„ë³„ ì˜ˆì™¸ ì²˜ë¦¬

## 9. ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

- **ë¯¼ê° ì •ë³´**: args/kwargsì—ì„œ ë¯¼ê° ë°ì´í„° ë§ˆìŠ¤í‚¹
- **ì ‘ê·¼ ì œì–´**: API ì—”ë“œí¬ì¸íŠ¸ ì¸ì¦/ê¶Œí•œ ë¶€ì—¬
- **ê°ì‚¬ ë¡œê·¸**: ëª¨ë“  íƒœìŠ¤í¬ ì‹¤í–‰ ê¸°ë¡ ìœ ì§€

## 10. ì‚¬ìš© ì˜ˆì‹œ

### AI íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í”Œë¡œìš°

```python
# 1. íŒŒì´í”„ë¼ì¸ ì‹œì‘
pipeline_id = create_ai_pipeline({
    "text": "ë¶„ì„í•  í…ìŠ¤íŠ¸",
    "options": {"model": "gpt-4"}
})

# 2. ì‹¤ì‹œê°„ ìƒíƒœ ì¶”ì 
while True:
    status = get_pipeline_status(pipeline_id)
    if status.overall_state in ['SUCCESS', 'FAILURE']:
        break
    time.sleep(1)

# 3. ê²°ê³¼ í™•ì¸
final_result = status.tasks[-1].result
```

ì´ ì•„í‚¤í…ì²˜ëŠ” Celeryì˜ ë¶„ì‚° íŠ¹ì„±ê³¼ ë°ì´í„°ë² ì´ìŠ¤ì˜ ì˜ì†ì„±ì„ ê²°í•©í•˜ì—¬ í™•ì¥ ê°€ëŠ¥í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” íƒœìŠ¤í¬ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì„ ì œê³µí•©ë‹ˆë‹¤.