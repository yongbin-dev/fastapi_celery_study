"""ë°°ì¹˜ íŒŒì´í”„ë¼ì¸ Celery íƒœìŠ¤í¬

ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ëŠ” Celery íƒœìŠ¤í¬ ëª¨ìŒ
PDF íŒŒì¼ì„ ì´ë¯¸ì§€ë¡œ ë³€í™˜ í›„ ë°°ì¹˜ ì²˜ë¦¬ë„ ì§€ì›
"""

import uuid
from typing import Any, Dict, List

from celery import group
from celery_app import celery_app
from shared.core.database import get_db_manager
from shared.core.logging import get_logger
from shared.pipeline.exceptions import RetryableError
from shared.schemas.common import ImageResponse
from shared.service.common_service import CommonService

logger = get_logger(__name__)


@celery_app.task(
    bind=True,
    name="pipeline.process_chunk",
    max_retries=3,
    autoretry_for=(ConnectionError, TimeoutError, RetryableError),
    retry_backoff=True,
    retry_backoff_max=600,
    retry_jitter=True,
)
def process_chunk_task(
    self,
    batch_id: str,
    chunk_index: int,
    image_response_list: List[ImageResponse],
    options: Dict[str, Any],
) -> Dict[str, Any]:
    """ì²­í¬ ë‹¨ìœ„ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬
    Args:
        self: Celery task instance
        batch_id: ë°°ì¹˜ ID
        chunk_index: ì²­í¬ ì¸ë±ìŠ¤
        image_response_dicts: ImageResponse dict ëª©ë¡
        options: íŒŒì´í”„ë¼ì¸ ì˜µì…˜
    Returns:
        ì²­í¬ ì²˜ë¦¬ ê²°ê³¼ (ì„±ê³µ/ì‹¤íŒ¨ ì´ë¯¸ì§€ ìˆ˜)
    """
    from shared.repository.crud.sync_crud.batch_execution import batch_execution_crud

    # pipeline_tasksì—ì„œ start_pipeline import
    from .pipeline_tasks import start_pipeline

    logger.info(
        f"ì²­í¬ ì²˜ë¦¬ ì‹œì‘: batch_id={batch_id}, chunk={chunk_index}, "
        f"images={len(image_response_list)}"
    )

    completed_count = 0
    failed_count = 0
    results = []

    # ê° ì´ë¯¸ì§€ì— ëŒ€í•´ ê°œë³„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    for idx, image_response in enumerate(image_response_list):
        try:
            # ê°œë³„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            context_id = start_pipeline(image_response, batch_id, options)
            file_path = image_response.private_img
            completed_count += 1
            results.append(
                {
                    "file_path": file_path,
                    "context_id": context_id,
                    "status": "success",
                }
            )

            logger.info(
                f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì™„ë£Œ: batch={batch_id}, chunk={chunk_index}, "
                f"idx={idx}, file={file_path}"
            )

        except Exception as e:
            failed_count += 1
            results.append(
                {
                    "file_path": image_response.private_img,
                    "status": "failed",
                    "error": str(e),
                }
            )

            logger.error(
                f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: batch={batch_id}, chunk={chunk_index}, "
                f"idx={idx}, file={file_path}, error={str(e)}"
            )

    # ë°°ì¹˜ í†µê³„ ì—…ë°ì´íŠ¸
    with get_db_manager().get_sync_session() as session:
        if session:
            batch_execution = batch_execution_crud.get_by_batch_id(
                session, batch_id=batch_id
            )

            if batch_execution:
                # ì™„ë£Œ/ì‹¤íŒ¨ ì´ë¯¸ì§€ ìˆ˜ ì—…ë°ì´íŠ¸
                if completed_count > 0:
                    batch_execution_crud.increment_completed_images(
                        session, batch_execution=batch_execution, count=completed_count
                    )

                if failed_count > 0:
                    batch_execution_crud.increment_failed_images(
                        session, batch_execution=batch_execution, count=failed_count
                    )

                # ì²­í¬ ì™„ë£Œ í‘œì‹œ
                batch_execution_crud.increment_completed_chunks(
                    session, batch_execution=batch_execution
                )

    logger.info(
        f"ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ: batch_id={batch_id}, chunk={chunk_index}, "
        f"completed={completed_count}, failed={failed_count}"
    )

    return {
        "batch_id": batch_id,
        "chunk_index": chunk_index,
        "completed": completed_count,
        "failed": failed_count,
        "results": results,
    }


def start_batch_pipeline(
    batch_name: str,
    image_response_list: List[ImageResponse],
    options: Dict[str, Any],
    chunk_size: int = 10,
    initiated_by: str = "api_server",
) -> str:
    """ë°°ì¹˜ íŒŒì´í”„ë¼ì¸ ì‹œì‘
    Args:
        batch_name: ë°°ì¹˜ ì´ë¦„
        image_response_list: ì²˜ë¦¬í•  ImageResponse ê°ì²´ ëª©ë¡
        options: íŒŒì´í”„ë¼ì¸ ì˜µì…˜
        chunk_size: ì²­í¬ë‹¹ ì´ë¯¸ì§€ ìˆ˜
        initiated_by: ì‹œì‘í•œ ì‚¬ìš©ì/ì‹œìŠ¤í…œ
    Returns:
        batch_id: ë°°ì¹˜ ê³ ìœ  ID
    """
    from shared.repository.crud.sync_crud.batch_execution import batch_execution_crud

    # 1. Batch ID ìƒì„±
    batch_id = str(uuid.uuid4())

    # 2. DBì— BatchExecution ìƒì„±
    with get_db_manager().get_sync_session() as session:
        if not session:
            raise RuntimeError("DB ì„¸ì…˜ ìƒì„± ì‹¤íŒ¨")

        batch_execution_crud.create_batch_execution(
            db=session,
            batch_id=batch_id,
            batch_name=batch_name,
            total_images=len(image_response_list),
            chunk_size=chunk_size,
            initiated_by=initiated_by,
            options=options,
        )

    # 3. ImageResponse ê°ì²´ë¥¼ dictë¡œ ë³€í™˜ (Celery ì§ë ¬í™”ë¥¼ ìœ„í•´)
    image_response_dicts = [
        {
            "public_img": img.public_img,
            "private_img": img.private_img,
        }
        for img in image_response_list
    ]

    # 4. íŒŒì¼ ê²½ë¡œë¥¼ ì²­í¬ë¡œ ë¶„í• 
    chunks = []
    for i in range(0, len(image_response_dicts), chunk_size):
        chunk_files = image_response_dicts[i : i + chunk_size]
        chunks.append(chunk_files)

    logger.info(
        f"ë°°ì¹˜ íŒŒì´í”„ë¼ì¸ ì‹œì‘: batch_id={batch_id}, "
        f"total_images={len(image_response_list)}, chunks={len(chunks)}"
    )

    # 5. ê° ì²­í¬ë¥¼ ë…ë¦½ì ì¸ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰
    # groupì„ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬
    chunk_tasks = group(
        process_chunk_task.s(
            batch_id=batch_id,
            chunk_index=idx,
            image_response_dicts=chunk_files,
            options=options,
        )
        for idx, chunk_files in enumerate(chunks)
    )

    # 6. ë°°ì¹˜ ì‹œì‘ ìƒíƒœë¡œ ë³€ê²½
    with get_db_manager().get_sync_session() as session:
        if session:
            batch_execution = batch_execution_crud.get_by_batch_id(
                session, batch_id=batch_id
            )
            if batch_execution:
                batch_execution.start_execution()
                session.commit()

    # 7. ë¹„ë™ê¸° ì‹¤í–‰
    chunk_tasks.apply_async()

    logger.info(f"ë°°ì¹˜ íƒœìŠ¤í¬ ì‹œì‘ë¨: batch_id={batch_id}")

    return batch_id


@celery_app.task(
    bind=True,
    name="pipeline.process_pdf_batch",
    max_retries=3,
    autoretry_for=(ConnectionError, TimeoutError, RetryableError),
    retry_backoff=True,
    retry_backoff_max=600,
    retry_jitter=True,
)
def process_pdf_batch_task(
    self,
    pdf_file_bytes: bytes,
    original_filename: str,
    options: Dict[str, Any] = {},
    chunk_size: int = 10,
) -> str:
    """PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ í›„ ë°°ì¹˜ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (Celery Task)

    Args:
        self: Celery task instance
        pdf_file_bytes: PDF íŒŒì¼ ë°”ì´íŠ¸ ë°ì´í„°
        original_filename: ì›ë³¸ íŒŒì¼ëª…
        options: íŒŒì´í”„ë¼ì¸ ì˜µì…˜
        chunk_size: ì²­í¬ë‹¹ ì´ë¯¸ì§€ ìˆ˜

    Returns:
        batch_id: ë°°ì¹˜ ê³ ìœ  ID
    """
    import asyncio

    logger.info(f"ğŸ“„ PDF ë°°ì¹˜ ì²˜ë¦¬ íƒœìŠ¤í¬ ì‹œì‘: filename={original_filename}")

    try:
        # 1. PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
        async def convert_pdf_to_images():
            common_service = CommonService()
            image_response_list = await common_service.save_pdf(
                original_filename=original_filename, pdf_file_bytes=pdf_file_bytes
            )
            return image_response_list

        # asyncioë¡œ PDF ë³€í™˜ ì‹¤í–‰
        image_response_list = asyncio.run(convert_pdf_to_images())

        logger.info(f"âœ… PDF ë³€í™˜ ì™„ë£Œ: {len(image_response_list)}ê°œ ì´ë¯¸ì§€ ìƒì„±")

        # 2. ë°°ì¹˜ íŒŒì´í”„ë¼ì¸ ì‹œì‘
        batch_name = f"pdf_{original_filename}_{uuid.uuid4().hex[:8]}"
        batch_id = start_batch_pipeline(
            batch_name=batch_name,
            image_response_list=image_response_list,
            options=options,
            chunk_size=chunk_size,
            initiated_by="pdf_batch_task",
        )

        logger.info(
            f"ğŸš€ PDF ë°°ì¹˜ íŒŒì´í”„ë¼ì¸ ì‹œì‘: batch_id={batch_id}, "
            f"images={len(image_response_list)}"
        )

        return batch_id

    except Exception as e:
        logger.error(
            f"âŒ PDF ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: filename={original_filename}, error={str(e)}"
        )
        raise


def start_batch_pipeline_from_pdf(
    pdf_file_bytes: bytes,
    original_filename: str,
    options: Dict[str, Any] = {},
    chunk_size: int = 10,
) -> str:
    """PDF íŒŒì¼ì„ ë°›ì•„ì„œ ë°°ì¹˜ íŒŒì´í”„ë¼ì¸ ì‹œì‘ (ë™ê¸° í•¨ìˆ˜)

    PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•œ í›„ ë°°ì¹˜ íŒŒì´í”„ë¼ì¸ì„ ì‹œì‘í•©ë‹ˆë‹¤.
    Celery íƒœìŠ¤í¬ë¡œ ë¹„ë™ê¸° ì‹¤í–‰ë©ë‹ˆë‹¤.

    Args:
        pdf_file_bytes: PDF íŒŒì¼ ë°”ì´íŠ¸ ë°ì´í„°
        original_filename: ì›ë³¸ íŒŒì¼ëª…
        options: íŒŒì´í”„ë¼ì¸ ì˜µì…˜
        chunk_size: ì²­í¬ë‹¹ ì´ë¯¸ì§€ ìˆ˜

    Returns:
        task_id: Celery íƒœìŠ¤í¬ ID (ê²°ê³¼ ì¡°íšŒìš©)
    """
    logger.info(f"ğŸ“„ PDF ë°°ì¹˜ íŒŒì´í”„ë¼ì¸ ì‹œì‘ ìš”ì²­: filename={original_filename}")

    # Celery íƒœìŠ¤í¬ë¡œ ë¹„ë™ê¸° ì‹¤í–‰
    result = process_pdf_batch_task.apply_async(
        kwargs={
            "pdf_file_bytes": pdf_file_bytes,
            "original_filename": original_filename,
            "options": options,
            "chunk_size": chunk_size,
        }
    )

    logger.info(
        f"ğŸ“¤ PDF ë°°ì¹˜ íƒœìŠ¤í¬ ì „ì†¡ ì™„ë£Œ: task_id={result.id}, "
        f"filename={original_filename}"
    )

    return result.id
