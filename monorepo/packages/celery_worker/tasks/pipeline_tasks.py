"""íŒŒì´í”„ë¼ì¸ Celery íƒœìŠ¤í¬

ê° ìŠ¤í…Œì´ì§€ë¥¼ Celery íƒœìŠ¤í¬ë¡œ ë˜í•‘í•˜ì—¬ ë¹„ë™ê¸° ì‹¤í–‰ ë° ì¬ì‹œë„ ì§€ì›
"""

import time
import uuid
from typing import Any, Dict, Optional

from celery import chain
from celery_app import celery_app
from shared.core.database import get_db_manager
from shared.core.logging import get_logger
from shared.pipeline.cache import get_pipeline_cache_service
from shared.pipeline.context import PipelineContext
from shared.pipeline.exceptions import RetryableError
from shared.schemas.common import ImageResponse
from shared.schemas.enums import ProcessStatus
from tasks.stages.llm_stage import LLMStage

from .stages.ocr_stage import OCRStage

logger = get_logger(__name__)

# PipelineCacheService ì¸ìŠ¤í„´ìŠ¤
cache_service = get_pipeline_cache_service()


# ê° ë‹¨ê³„ë³„ Celery íƒœìŠ¤í¬
@celery_app.task(
    bind=True,
    name="pipeline.ocr_stage",
    max_retries=3,
    autoretry_for=(ConnectionError, TimeoutError, RetryableError),
    retry_backoff=True,
    retry_backoff_max=600,
    retry_jitter=True,
)
def ocr_stage_task(self, context_dict: Dict[str, str]) -> Dict[str, str]:
    """OCR ë‹¨ê³„ ì‹¤í–‰

    Args:
        self: Celery task instance
        context_dict: batch_idì™€ chain_idë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬

    Returns:
        ì»¨í…ìŠ¤íŠ¸ ID (ë‹¤ìŒ ë‹¨ê³„ë¡œ ì „ë‹¬)
    """
    # ë”•ì…”ë„ˆë¦¬ì—ì„œ batch_idì™€ chain_id ì¶”ì¶œ
    batch_id = context_dict["batch_id"]
    chain_id = context_dict["chain_id"]

    # Redisì—ì„œ context ë¡œë“œ
    context = cache_service.load_context(batch_id, chain_id)

    # ì·¨ì†Œ ìƒíƒœ í™•ì¸
    if context.status == ProcessStatus.REVOKED:
        logger.warning(f"âš ï¸ ì‘ì—… {chain_id}ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤ (OCR stage)")
        raise Exception(f"ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤: {chain_id}")

    # OCR ì‹¤í–‰ (ë™ê¸°ë¡œ ì‹¤í–‰ - run_sync í•„ìš” ì—†ìŒ, async í•¨ìˆ˜ë¥¼ ì§ì ‘ í˜¸ì¶œ)
    import asyncio

    stage = OCRStage()
    context = asyncio.run(stage.run(context))

    # Redisì— ì €ì¥
    cache_service.save_context(context)

    return {"batch_id": batch_id, "chain_id": chain_id}  # ë‹¤ìŒ ë‹¨ê³„ë¡œ ì „ë‹¬


@celery_app.task(
    bind=True,
    name="pipeline.llm_stage",
    max_retries=3,
    autoretry_for=(ConnectionError, TimeoutError, RetryableError),
    retry_backoff=True,
    retry_backoff_max=600,
)
def llm_stage_task(self, context_dict: Dict[str, str]) -> Dict[str, str]:
    """LLM ë¶„ì„ ë‹¨ê³„ ì‹¤í–‰

    Args:
        self: Celery task instance
        context_id: ì»¨í…ìŠ¤íŠ¸ ID

    Returns:
        ì»¨í…ìŠ¤íŠ¸ ID
    """

    # ë”•ì…”ë„ˆë¦¬ì—ì„œ batch_idì™€ chain_id ì¶”ì¶œ
    batch_id = context_dict["batch_id"]
    chain_id = context_dict["chain_id"]

    # Redisì—ì„œ context ë¡œë“œ
    context = cache_service.load_context(batch_id, chain_id)

    # ì·¨ì†Œ ìƒíƒœ í™•ì¸
    if context.status == ProcessStatus.REVOKED:
        logger.warning(f"âš ï¸ ì‘ì—… {chain_id}ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤ (LLM stage)")
        raise Exception(f"ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤: {chain_id}")

    # OCR ì‹¤í–‰ (ë™ê¸°ë¡œ ì‹¤í–‰ - run_sync í•„ìš” ì—†ìŒ, async í•¨ìˆ˜ë¥¼ ì§ì ‘ í˜¸ì¶œ)
    import asyncio

    stage = LLMStage()
    context = asyncio.run(stage.run(context))

    logger.info(f"{chain_id} sleep start")
    time.sleep(5)
    logger.info(f"{chain_id} sleep end")

    # Redisì— ì €ì¥
    cache_service.save_context(context)
    return {"batch_id": batch_id, "chain_id": chain_id}  # ë‹¤ìŒ ë‹¨ê³„ë¡œ ì „ë‹¬


@celery_app.task(
    bind=True,
    name="pipeline.finish_stage_task",
    max_retries=3,
    autoretry_for=(ConnectionError, TimeoutError, RetryableError),
    retry_backoff=True,
    retry_backoff_max=600,
)
def finish_stage_task(self, context_dict: Dict[str, str]) -> Dict[str, str]:
    """LLM ë¶„ì„ ë‹¨ê³„ ì‹¤í–‰

    Args:
        self: Celery task instance
        context_id: ì»¨í…ìŠ¤íŠ¸ ID

    Returns:
        ì»¨í…ìŠ¤íŠ¸ ID
    """

    # ë”•ì…”ë„ˆë¦¬ì—ì„œ batch_idì™€ chain_id ì¶”ì¶œ
    batch_id = context_dict["batch_id"]
    chain_id = context_dict["chain_id"]

    # Redisì—ì„œ context ë¡œë“œ
    context = cache_service.load_context(batch_id, chain_id)

    context.status = ProcessStatus.SUCCESS
    # Redisì— ì €ì¥
    cache_service.save_context(context)

    return {"batch_id": batch_id, "chain_id": chain_id}  # ë‹¤ìŒ ë‹¨ê³„ë¡œ ì „ë‹¬


@celery_app.task(bind=True, name="tasks.start_pipeline")
def start_pipeline_task(
    self, image_response: ImageResponse, batch_id: str, options: Dict[str, Any]
) -> str:
    """íŒŒì´í”„ë¼ì¸ ì‹œì‘ (Celery Task)"""
    return start_pipeline(image_response, batch_id, options)


# íŒŒì´í”„ë¼ì¸ ì‹œì‘ í•¨ìˆ˜ (ë¹„ë™ê¸° chain ë°©ì‹)
def start_pipeline(
    image_response: ImageResponse, batch_id: Optional[str], options: Dict[str, Any] = {}
) -> str:
    """íŒŒì´í”„ë¼ì¸ ì‹œì‘ (ë¹„ë™ê¸° chain ë°©ì‹)

    Args:
        image_response: ì´ë¯¸ì§€ ì‘ë‹µ ê°ì²´
        batch_id: ë°°ì¹˜ ID
        options: íŒŒì´í”„ë¼ì¸ ì˜µì…˜

    Returns:
        context_id: íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¶”ì  ID (=chain_id)
    """
    # 1. Chain ID ìƒì„±
    chain_id = str(uuid.uuid4())

    # 2. DBì— ChainExecution ìƒì„±
    from shared.repository.crud.sync_crud.chain_execution import chain_execution_crud

    # batch_idê°€ ë¹ˆ ë¬¸ìì—´ì´ë©´ Noneìœ¼ë¡œ ë³€í™˜ (ì™¸ë˜ í‚¤ ì œì•½ ì¡°ê±´ ìœ„ë°˜ ë°©ì§€)
    batch_id = batch_id if batch_id else None

    with get_db_manager().get_sync_session() as session:
        if not session:
            raise RuntimeError("DB ì„¸ì…˜ ìƒì„± ì‹¤íŒ¨")

        chain_exec = chain_execution_crud.create_chain_execution(
            db=session,
            chain_id=chain_id,
            batch_id=batch_id,
            chain_name="workflow",
            total_tasks=2,  # OCR, LLM
            initiated_by="api_server",
            input_data={"file_path": image_response.private_img, "options": options},
        )

        # 3. Context ìƒì„± ë° Redis ì €ì¥
        context = PipelineContext(
            batch_id=batch_id or "",
            chain_id=chain_id,
            private_img=image_response.private_img,
            public_file_path=image_response.public_img,
            options=options,
        )

        cache_service.save_context(context)

        # 4. Celery chainìœ¼ë¡œ ë‹¨ê³„ ì—°ê²°
        workflow = chain(
            ocr_stage_task.s(
                {"batch_id": context.batch_id, "chain_id": context.chain_id}
            ),
            llm_stage_task.s(),
            finish_stage_task.s(),
        )

        # 5. ë¹„ë™ê¸° ì‹¤í–‰
        result = workflow.apply_async()

        # 6. Celery task IDë¥¼ DBì— ì €ì¥
        chain_execution_crud.update_celery_task_id(
            db=session, chain_execution=chain_exec, celery_task_id=result.id
        )
        logger.info(f"âœ… Celery task ID ì €ì¥ ì™„ë£Œ: {result.id} (chain_id: {chain_id})")

        return context.chain_id


def start_pipeline_sync(
    image_response: ImageResponse, batch_id: Optional[str], options: Dict[str, Any] = {}
) -> str:
    """íŒŒì´í”„ë¼ì¸ ì‹œì‘ (ë™ê¸° ìˆœì°¨ ì‹¤í–‰ ë°©ì‹)

    ê° ìŠ¤í…Œì´ì§€ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ì—¬ OCR â†’ LLM ìˆœì„œë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.
    ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ê° í•­ëª©ì´ ì™„ì „íˆ ì²˜ë¦¬ëœ í›„ ë‹¤ìŒ í•­ëª©ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.

    Args:
        image_response: ì´ë¯¸ì§€ ì‘ë‹µ ê°ì²´
        batch_id: ë°°ì¹˜ ID
        options: íŒŒì´í”„ë¼ì¸ ì˜µì…˜

    Returns:
        context_id: íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¶”ì  ID (=chain_id)
    """
    import asyncio

    # 1. Chain ID ìƒì„±
    chain_id = str(uuid.uuid4())

    # 2. DBì— ChainExecution ìƒì„±
    from shared.repository.crud.sync_crud.chain_execution import chain_execution_crud

    # batch_idê°€ ë¹ˆ ë¬¸ìì—´ì´ë©´ Noneìœ¼ë¡œ ë³€í™˜
    batch_id = batch_id if batch_id else None

    with get_db_manager().get_sync_session() as session:
        if not session:
            raise RuntimeError("DB ì„¸ì…˜ ìƒì„± ì‹¤íŒ¨")

        chain_exec = chain_execution_crud.create_chain_execution(
            db=session,
            chain_id=chain_id,
            batch_id=batch_id,
            chain_name="workflow",
            total_tasks=2,  # OCR, LLM
            initiated_by="api_server",
            input_data={"file_path": image_response.private_img, "options": options},
        )

    # 3. Context ìƒì„± ë° Redis ì €ì¥
    context = PipelineContext(
        batch_id=batch_id or "",
        chain_id=chain_id,
        private_img=image_response.private_img,
        public_file_path=image_response.public_img,
        options=options,
    )

    cache_service.save_context(context)

    try:
        # 4. ê° ìŠ¤í…Œì´ì§€ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
        logger.info(f"ğŸš€ íŒŒì´í”„ë¼ì¸ ì‹œì‘ (ë™ê¸°): chain_id={chain_id}")

        # OCR Stage
        logger.info(f"ğŸ“¸ OCR Stage ì‹œì‘: chain_id={chain_id}")
        ocr_stage = OCRStage()
        context = asyncio.run(ocr_stage.run(context))
        cache_service.save_context(context)
        logger.info(f"âœ… OCR Stage ì™„ë£Œ: chain_id={chain_id}")

        # LLM Stage
        logger.info(f"ğŸ¤– LLM Stage ì‹œì‘: chain_id={chain_id}")
        llm_stage = LLMStage()
        context = asyncio.run(llm_stage.run(context))
        cache_service.save_context(context)
        logger.info(f"âœ… LLM Stage ì™„ë£Œ: chain_id={chain_id}")

        # ì™„ë£Œ ì²˜ë¦¬
        context.status = ProcessStatus.SUCCESS
        cache_service.save_context(context)
        logger.info(f"ğŸ‰ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: chain_id={chain_id}")

    except Exception as e:
        logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: chain_id={chain_id}, error={str(e)}")
        context.status = ProcessStatus.FAILURE
        cache_service.save_context(context)
        raise

    return context.chain_id
