[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "ml-server"
version = "0.1.0"
description = "ML_SERVER"
requires-python = ">=3.12"
dependencies = [
    "shared",  # 내부 공통 패키지 (fastapi 포함)
    "uvicorn[standard]>=0.37.0",
    "python-multipart>=0.0.20",  # Form 데이터 처리
    "python-Levenshtein>=0.23.0",
    "scikit-learn>=1.7.2",
    "scikit-image>=0.25.0",
    "celery>=5.3.0",  # Celery 태스크 호출
    "kombu>=5.3.0",
    "vllm==0.6.6",
    "bentoml>=1.3.0",  # BentoML 모델 서빙
    # grpcio-reflection은 protobuf 4.x를 요구하므로 제외 (디버깅용이므로 필수 아님)
]


[project.optional-dependencies]

# ML 공통 베이스 (torch + 기본 라이브러리)
ml-base = [
    "torch>=2.2.0",
    "torchvision>=0.17.0",
    "opencv-python>=4.9.0",
    "Pillow>=10.2.0",
    "numpy<2.0.0", # vllm의 요구 사항을 명시적으로 추가
    "scipy>=1.13.0",
    
]


# OCR CPU 버전 (경량화)
ocr-cpu = [
    "paddleocr>=2.8.0,<3.0.0",
    "paddlepaddle>=2.6.2,<3.0.0",  # CPU 버전 (GPU 버전보다 훨씬 가벼움)
    "easyocr>=1.7.2",
    "ml-server[ml-base]",
]

# OCR GPU 버전
ocr-gpu = [
    "paddleocr>=2.8.0,<3.0.0",
    "paddlepaddle-gpu>=2.6.2,<3.0.0",  # GPU 버전
    "easyocr>=1.7.2",
    "ml-server[ml-base]",
]

[tool.uv.extra-build-dependencies]
# uv는 vllm이 PyTorch에 의존한다는 것을 빌드 단계에서 인지하고
# 위에 명시된 "torch"를 사용하여 vllm을 빌드하도록 돕습니다.
# "match-runtime = true"는 [project].dependencies에 있는 torch를 사용하도록 지정합니다.
vllm = [
    { requirement = "torch", match-runtime = true }
]

[tool.hatch.build.targets.wheel]
packages = ["ml_app"]