"""
ML Server Main Application
AI/ML ëª¨ë¸ ì¶”ë¡ ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë²„
"""

from contextlib import asynccontextmanager

from fastapi import FastAPI
from shared.config import settings
from shared.core.logging import get_logger

logger = get_logger(__name__)

# gRPC í™œì„±í™” ì—¬ë¶€
USE_GRPC = settings.USE_GRPC == "true"


@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬"""

    logger.info(f"ğŸš€ ML ì„œë²„ ì‹œì‘ : ${settings.ML_SERVER_PORT}")

    # gRPC ì„œë²„ ì‹œì‘ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
    # from ml_app.services.grpc_services.server import serve

    # grpc_task = asyncio.create_task(serve())
    # logger.info("âœ… gRPC ì„œë²„ íƒœìŠ¤í¬ ì‹œì‘ (í¬íŠ¸: 50051)")

    # BentoML ì„œë²„ ì‹œì‘

    yield

    # ì¢…ë£Œ ì‹œ
    logger.info("ğŸ›‘ ML ì„œë²„ ì¢…ë£Œ")

    # gRPC ì¢…ë£Œ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
    # if grpc_task:
    #     grpc_task.cancel()
    #     try:
    #         await grpc_task
    #     except asyncio.CancelledError:
    #         logger.info("gRPC ì„œë²„ íƒœìŠ¤í¬ ì¢…ë£Œ")


app = FastAPI(
    title="ML Model Server",
    version="1.0.0",
    description="AI/ML ëª¨ë¸ ì¶”ë¡  ì„œë²„ - OCR, LLM ë“±",
    lifespan=lifespan,
)
