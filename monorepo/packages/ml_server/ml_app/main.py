"""
ML Server Main Application
AI/ML ëª¨ë¸ ì¶”ë¡ ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë²„
"""

import asyncio
from contextlib import asynccontextmanager

from fastapi import FastAPI
from shared.config import settings
from shared.core.logging import get_logger

logger = get_logger(__name__)

# gRPC í™œì„±í™” ì—¬ë¶€
USE_GRPC = settings.USE_GRPC == "true"


@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬"""

    logger.info("ğŸš€ ML ì„œë²„ ì‹œì‘")

    bentoml_process = None

    # gRPC ì„œë²„ ì‹œì‘ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
    # if USE_GRPC:
    #     from ml_app.grpc_services.server import serve
    #     grpc_task = asyncio.create_task(serve())
    #     logger.info("âœ… gRPC ì„œë²„ íƒœìŠ¤í¬ ì‹œì‘ (í¬íŠ¸: 50051)")
    # else:
    #     logger.info("âš ï¸  gRPC ë¹„í™œì„±í™” (USE_GRPC=false)")

    # BentoML ì„œë²„ ì‹œì‘
    try:
        # í”„ë¡œì„¸ìŠ¤ ìƒì„±
        bentoml_process = await asyncio.create_subprocess_exec(
            "bentoml",
            "serve",
            "ml_app.bentoml_services:OCRBentoService",
            "--host",
            "0.0.0.0",
            "--port",
            "50052",
            "--api-workers",
            "1",
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )

        logger.info("âœ… BentoML ì„œë²„ ì‹œì‘ë¨ (í¬íŠ¸: 50052)")

        # ë¡œê¹… íƒœìŠ¤í¬ ì‹œì‘
        async def log_output(stream, prefix):
            try:
                async for line in stream:
                    logger.info(f"{prefix}: {line.decode().strip()}")
            except Exception:
                pass

        asyncio.create_task(log_output(bentoml_process.stdout, "BentoML"))
        asyncio.create_task(log_output(bentoml_process.stderr, "BentoML-ERR"))

    except Exception as e:
        logger.error(f"âŒ BentoML ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")

    yield

    # ì¢…ë£Œ ì‹œ
    logger.info("ğŸ›‘ ML ì„œë²„ ì¢…ë£Œ")

    # gRPC ì¢…ë£Œ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
    # if grpc_task:
    #     grpc_task.cancel()
    #     try:
    #         await grpc_task
    #     except asyncio.CancelledError:
    #         logger.info("gRPC ì„œë²„ íƒœìŠ¤í¬ ì¢…ë£Œ")

    # BentoML í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
    if bentoml_process:
        logger.info("BentoML ì„œë²„ ì¢…ë£Œ ì¤‘...")
        try:
            # ì •ìƒ ì¢…ë£Œ ì‹œë„ (SIGTERM)
            bentoml_process.terminate()

            # 5ì´ˆ ëŒ€ê¸°
            try:
                await asyncio.wait_for(bentoml_process.wait(), timeout=5.0)
                logger.info("âœ… BentoML ì„œë²„ ì •ìƒ ì¢…ë£Œ")
            except asyncio.TimeoutError:
                # íƒ€ì„ì•„ì›ƒ ì‹œ ê°•ì œ ì¢…ë£Œ (SIGKILL)
                logger.warning("BentoML ì„œë²„ê°€ ì‘ë‹µí•˜ì§€ ì•ŠìŒ. ê°•ì œ ì¢…ë£Œ...")
                bentoml_process.kill()
                await bentoml_process.wait()
                logger.info("âœ… BentoML ì„œë²„ ê°•ì œ ì¢…ë£Œ")
        except Exception as e:
            logger.error(f"BentoML ì„œë²„ ì¢…ë£Œ ì‹¤íŒ¨: {e}")


app = FastAPI(
    title="ML Model Server",
    version="1.0.0",
    description="AI/ML ëª¨ë¸ ì¶”ë¡  ì„œë²„ - OCR, LLM ë“±",
    lifespan=lifespan,
)
